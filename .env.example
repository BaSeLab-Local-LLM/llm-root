# ==============================================================================
#                    LOCAL LLM DEPLOYMENT CONFIGURATION
# ==============================================================================
# This file controls all hardware resources, service settings, and secrets.
# Copy this file to .env and modify as needed.
# Changes require: docker compose down && docker compose up -d
# ==============================================================================

# ================================ GPU SETTINGS ================================
# NVIDIA GPU device selection
# Options: "0" (first GPU), "0,1" (multi-GPU), "none" (disable GPU)
NVIDIA_VISIBLE_DEVICES=0

# vLLM GPU memory utilization ratio (0.0 - 1.0)
# Maximum preset: 0.85 (uses ~10.2GB of 12GB VRAM)
# Lower this value if you experience OOM errors
VLLM_GPU_MEMORY_UTILIZATION=0.85

# Maximum context length (affects VRAM usage)
# Options: 2048, 4096, 8192, 16384, 32768 (higher = more VRAM)
VLLM_MAX_MODEL_LEN=4096

# Data type for model weights
# Options: "auto", "float16", "bfloat16"
VLLM_DTYPE=auto

# ================================ CPU SETTINGS ================================
# CPU cores allocation per container
# Adjust based on your system's CPU cores

# vLLM container CPU limit (handles model inference)
VLLM_CPU_LIMIT=12

# LiteLLM container CPU limit (handles API gateway)
LITELLM_CPU_LIMIT=4

# PostgreSQL container CPU limit
POSTGRES_CPU_LIMIT=2

# Frontend container CPU limit (Nginx serving)
FRONTEND_CPU_LIMIT=1

# OpenMP thread count for parallel operations
OMP_NUM_THREADS=8

# NumExpr thread count for numerical operations
NUMEXPR_MAX_THREADS=8

# Intel MKL thread count (if applicable)
MKL_NUM_THREADS=8

# ================================ MEMORY SETTINGS =============================
# RAM allocation per container
# Adjust based on your system's total RAM

# vLLM container memory limit (model + KV cache)
VLLM_MEMORY_LIMIT=24g

# LiteLLM container memory limit
LITELLM_MEMORY_LIMIT=2g

# PostgreSQL container memory limit
POSTGRES_MEMORY_LIMIT=2g

# Frontend container memory limit
FRONTEND_MEMORY_LIMIT=512m

# PostgreSQL shared buffers (typically 25% of POSTGRES_MEMORY_LIMIT)
POSTGRES_SHARED_BUFFERS=512MB

# ================================ VLLM ADVANCED ===============================
# Maximum concurrent sequences (affects throughput vs memory)
# Lower = less memory, higher = more concurrent users
VLLM_MAX_NUM_SEQS=64

# CPU memory offloading (GB) - uses RAM when VRAM is insufficient
# Set to 0 to disable, or 4-8 for hybrid CPU/GPU inference
VLLM_CPU_OFFLOAD_GB=0

# Tensor parallelism for multi-GPU setups
# Single GPU: 1, Multi-GPU: number of GPUs
VLLM_TENSOR_PARALLEL_SIZE=1

# Swap space for KV cache (GB)
VLLM_SWAP_SPACE=4

# ================================ PERFORMANCE OPTIMIZATION ====================
# Prefix Caching - reuse KV cache for common prefixes
# Highly recommended for multi-user scenarios with shared system prompts
VLLM_ENABLE_PREFIX_CACHING=true

# Chunked Prefill - process long prompts in chunks
# Improves TTFT (Time To First Token) for long inputs
VLLM_ENABLE_CHUNKED_PREFILL=true

# Max prefill tokens per chunk (only when chunked prefill enabled)
# Lower = better TTFT, Higher = better throughput
# Recommended: 512 (responsive), 1024 (balanced), 2048 (throughput)
VLLM_MAX_NUM_BATCHED_TOKENS=1024

# Quantization method for reduced VRAM usage
# Options: "none", "awq", "gptq", "squeezellm", "fp8"
# Note: Requires quantized model (e.g., Qwen2.5-7B-Instruct-AWQ)
VLLM_QUANTIZATION=none

# Speculative Decoding - use small draft model for faster inference
# Leave empty to disable, or specify draft model
# Example: Qwen/Qwen2.5-0.5B-Instruct
VLLM_SPECULATIVE_MODEL=

# Number of speculative tokens to predict (if speculative model enabled)
VLLM_NUM_SPECULATIVE_TOKENS=5

# KV Cache data type for memory optimization
# Options: "auto", "fp8", "fp8_e5m2", "fp8_e4m3"
VLLM_KV_CACHE_DTYPE=auto

# Guided Decoding Backend (for structured JSON output)
# Options: "outlines", "lm-format-enforcer"
VLLM_GUIDED_DECODING_BACKEND=outlines

# ================================ MODEL SETTINGS ==============================
# HuggingFace model identifier
# Examples:
#   - Qwen/Qwen2.5-3B-Instruct (recommended for 12GB VRAM)
#   - Qwen/Qwen2.5-7B-Instruct-AWQ (with quantization)
#   - mistralai/Mistral-7B-Instruct-v0.3
#   - microsoft/Phi-3-mini-4k-instruct
#   - meta-llama/Llama-3.2-3B-Instruct (requires HF_TOKEN)
VLLM_MODEL=Qwen/Qwen2.5-3B-Instruct

# HuggingFace token (only needed for gated models like Llama, Gemma)
# Get your token at: https://huggingface.co/settings/tokens
HF_TOKEN=

# Model cache directory (persisted in Docker volume)
HF_HOME=/root/.cache/huggingface

# ================================ NETWORK PORTS ===============================
# Exposed ports on host machine
# Change if ports conflict with existing services

# vLLM internal port (usually not exposed externally)
VLLM_PORT=8000

# LiteLLM API Gateway port (main API endpoint)
LITELLM_PORT=4000

# Frontend port (React app)
FRONTEND_PORT=3000

# PostgreSQL port (for debugging, can be disabled in production)
POSTGRES_PORT=5432

# ================================ DATABASE ====================================
# PostgreSQL credentials
POSTGRES_USER=llmadmin
POSTGRES_DB=litellm

# PostgreSQL password (CHANGE THIS! Generate with: openssl rand -hex 16)
POSTGRES_PASSWORD=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_16

# ================================ JWT SETTINGS ================================
# JWT signing secret (REQUIRED, minimum 32 characters)
# Generate with: openssl rand -hex 32
JWT_SECRET_KEY=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_32

# JWT token expiration time (hours)
JWT_EXPIRE_HOURS=24

# ================================ CORS SETTINGS ===============================
# Allowed origins for CORS (comma-separated)
# Example: http://localhost:3000,https://yourdomain.com
ALLOWED_ORIGINS=http://localhost:3000

# ================================ LITELLM SETTINGS ============================
# Master key for admin access (use this to access Admin UI)
# Generate with: echo "sk-master-$(openssl rand -hex 24)"
LITELLM_MASTER_KEY=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_32

# Salt key for hashing virtual keys (keep secret!)
# Generate with: openssl rand -hex 16
LITELLM_SALT_KEY=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_16

# Admin UI credentials (⚠ 반드시 변경하세요! 기본값 사용 금지!)
# Generate with: openssl rand -base64 18
LITELLM_UI_USERNAME=admin
LITELLM_UI_PASSWORD=CHANGE_ME_SET_SECURE_PASSWORD

# ================================ RATE LIMITING ===============================
# Default rate limits for new Virtual Keys
DEFAULT_RATE_LIMIT_RPM=60
DEFAULT_RATE_LIMIT_TPM=100000

# ================================ LOGGING =====================================
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# ==============================================================================
#                           QUICK REFERENCE
# ==============================================================================
# 
# Start services:
#    docker compose up -d
#
# Apply changes:
#    docker compose down && docker compose up -d
#
# Monitor GPU:
#    watch -n 1 nvidia-smi
#
# View logs:
#    docker compose logs -f vllm
#    docker compose logs -f litellm
#
# Access Admin UI:
#    http://localhost:4000/ui
#    Username: admin (or LITELLM_UI_USERNAME)
#    Password: (LITELLM_UI_PASSWORD)
#
# Test API:
#    curl http://localhost:4000/v1/chat/completions \
#      -H "Authorization: Bearer $LITELLM_MASTER_KEY" \
#      -H "Content-Type: application/json" \
#      -d '{"model": "local-llm", "messages": [{"role": "user", "content": "Hello"}]}'
#
# ==============================================================================
