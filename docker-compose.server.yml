version: "3.8"

# ==============================================================================
#                    SERVER LLM DEPLOYMENT - DOCKER COMPOSE
# ==============================================================================
# Qwen3-VL-32B-Instruct-FP8 전용 서버 배포 구성
# 서버 사양: Threadripper PRO 7975WX / DDR5 256GB / RTX PRO 6000 96GB
#
# Services:
#   - vllm: GPU-accelerated LLM inference server (FP8 + FP8 KV cache)
#   - postgres: Database for LiteLLM (API keys, usage tracking)
#   - litellm: API Gateway (OpenAI-compatible, key management)
#   - backend: FastAPI (auth, chat, file upload)
#   - frontend: React web UI (Nginx)
#
# 사용법:
#   docker compose -f docker-compose.server.yml --env-file .env.server up -d
# ==============================================================================

services:
  # ============================================================================
  #                              vLLM SERVICE
  # ============================================================================
  # Qwen3-VL-32B-Instruct-FP8 — FP8 모델 + FP8 KV 캐시
  # VRAM 사용: ~33GB(모델) + ~12GB(KV) + ~3GB(오버헤드) ≈ 48GB
  # Blackwell GPU 하드웨어 네이티브 FP8 가속 활용
  # ============================================================================
  vllm:
    image: vllm/vllm-openai:v0.11.1
    container_name: llm-vllm
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0}
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=${HF_HOME:-/root/.cache/huggingface}
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-4}
      - NUMEXPR_MAX_THREADS=${NUMEXPR_MAX_THREADS:-4}
      - MKL_NUM_THREADS=${MKL_NUM_THREADS:-4}
      - NCCL_P2P_DISABLE=1
    entrypoint: ["vllm", "serve"]
    command:
      - "${VLLM_MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}"
      # ── 컨텍스트 & 메모리 ──
      - "--max-model-len"
      - "${VLLM_MAX_MODEL_LEN:-8192}"
      - "--gpu-memory-utilization"
      - "${VLLM_GPU_MEMORY_UTILIZATION:-0.50}"
      - "--dtype"
      - "${VLLM_DTYPE:-auto}"
      - "--kv-cache-dtype"
      - "${VLLM_KV_CACHE_DTYPE:-auto}"
      - "--swap-space"
      - "${VLLM_SWAP_SPACE:-8}"
      # ── 동시성 & 배치 ──
      - "--max-num-seqs"
      - "${VLLM_MAX_NUM_SEQS:-16}"
      - "--max-num-batched-tokens"
      - "${VLLM_MAX_NUM_BATCHED_TOKENS:-2048}"
      # ── 병렬 처리 ──
      - "--tensor-parallel-size"
      - "${VLLM_TENSOR_PARALLEL_SIZE:-1}"
      # ── 성능 최적화 ──
      - "--enable-prefix-caching"
      - "--enable-chunked-prefill"
      # ── 비전 모델 ──
      - "--trust-remote-code"
      - "--limit-mm-per-prompt"
      - '{"image": 5}'
      # ── 네트워크 ──
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
    # vLLM은 내부 네트워크에서만 접근 가능 (LiteLLM을 통해서만 사용)
    # 디버깅이 필요한 경우 아래 주석을 해제하세요
    # ports:
    #   - "127.0.0.1:${VLLM_PORT:-8000}:8000"
    volumes:
      - vllm-cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          cpus: "${VLLM_CPU_LIMIT:-6}"
          memory: ${VLLM_MEMORY_LIMIT:-48g}
        reservations:
          devices:
            - driver: nvidia
              count: ${VLLM_GPU_COUNT:-1}
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      # 32B FP8 모델은 로딩에 시간이 걸림 (첫 실행 시 다운로드 포함 ~10분)
      start_period: 600s
    networks:
      - llm-network

  # ============================================================================
  #                           POSTGRESQL SERVICE
  # ============================================================================
  postgres:
    image: postgres:16-alpine
    container_name: llm-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-llmadmin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
      POSTGRES_DB: ${POSTGRES_DB:-litellm}
      PGDATA: /var/lib/postgresql/data/pgdata
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=${POSTGRES_SHARED_BUFFERS:-512MB}"
      - "-c"
      - "max_connections=100"
    # PostgreSQL: 항상 localhost에만 바인딩 (외부 접근 차단)
    # DB는 Docker 내부 네트워크로만 접근하므로 외부 노출 불필요
    ports:
      - "127.0.0.1:${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/01-init-schema.sql:ro
    deploy:
      resources:
        limits:
          cpus: "${POSTGRES_CPU_LIMIT:-1}"
          memory: ${POSTGRES_MEMORY_LIMIT:-2g}
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-llmadmin} -d ${POSTGRES_DB:-litellm}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - llm-network

  # ============================================================================
  #                         LITELLM PROXY SERVICE
  # ============================================================================
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: llm-litellm
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-llmadmin}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-litellm}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:?LITELLM_MASTER_KEY is required}
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY:-}
      # ⚠ 반드시 .env에서 변경하세요! 기본값(admin)은 보안 위험!
      - UI_USERNAME=${LITELLM_UI_USERNAME:-admin}
      - UI_PASSWORD=${LITELLM_UI_PASSWORD:?LITELLM_UI_PASSWORD is required}
      - STORE_MODEL_IN_DB=True
      # litellm-config.yaml에서 os.environ/ 으로 참조
      - LITELLM_MODEL_ID=openai/${VLLM_MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - VLLM_API_BASE=http://vllm:${VLLM_PORT:-8000}/v1
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-8192}
    command: >
      --config /app/config.yaml
      --port 4000
      --num_workers 4
    # LiteLLM은 내부 네트워크에서만 접근 가능 (백엔드를 통해서만 사용)
    # Admin UI 접근이 필요하면 아래 주석을 해제하세요 (반드시 127.0.0.1 바인딩)
    # ports:
    #   - "127.0.0.1:${LITELLM_PORT:-4000}:4000"
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    deploy:
      resources:
        limits:
          cpus: "${LITELLM_CPU_LIMIT:-1}"
          memory: ${LITELLM_MEMORY_LIMIT:-2g}
    depends_on:
      postgres:
        condition: service_healthy
      vllm:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/readiness')"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - llm-network

  # ============================================================================
  #                           BACKEND SERVICE (FastAPI)
  # ============================================================================
  # 비전 모델 최적화: 이미지 리사이즈/디코딩을 위해 CPU 3코어 배정
  # ============================================================================
  backend:
    build:
      context: ./submodules/backend
      dockerfile: Dockerfile
    container_name: llm-backend
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-llmadmin}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-litellm}
      - LITELLM_URL=http://litellm:${LITELLM_PORT:-4000}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - JWT_EXPIRE_HOURS=${JWT_EXPIRE_HOURS:-24}
      - DEBUG=${DEBUG:-false}
      # ALLOWED_ORIGINS: .env에 ALLOWED_ORIGINS가 설정되면 그 값 사용
      # 미설정 시 SERVER_IP + FRONTEND_PORT로 자동 생성
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://${SERVER_IP:-localhost}:${FRONTEND_PORT:-3000}}
      - MAX_REQUEST_BODY_SIZE=${MAX_REQUEST_BODY_SIZE:-50}
      - MAX_UPLOAD_FILE_SIZE=${MAX_UPLOAD_FILE_SIZE:-20}
      - MAX_IMAGE_DIMENSION=${MAX_IMAGE_DIMENSION:-1280}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-32768}
    ports:
      - "${BIND_ADDRESS:-127.0.0.1}:${BACKEND_PORT:-8080}:8000"
    deploy:
      resources:
        limits:
          cpus: "${BACKEND_CPU_LIMIT:-3}"
          memory: ${BACKEND_MEMORY_LIMIT:-4g}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - llm-network
    restart: unless-stopped

  # ============================================================================
  #                           SEED SERVICE (Run Once)
  # ============================================================================
  seed:
    build:
      context: ./submodules/backend
    container_name: llm-seed
    command: python /app/scripts/generate_keys.py
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-llmadmin}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-litellm}
      - LITELLM_URL=http://litellm:${LITELLM_PORT:-4000}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - DEFAULT_RATE_LIMIT_RPM=${DEFAULT_RATE_LIMIT_RPM:-10}
      - DEFAULT_RATE_LIMIT_TPM=${DEFAULT_RATE_LIMIT_TPM:-100000}
      - DEFAULT_STUDENT_MAX_BUDGET=${DEFAULT_STUDENT_MAX_BUDGET:-1.0}
      - ADMIN_RATE_LIMIT_RPM=${ADMIN_RATE_LIMIT_RPM:-1000}
      - ADMIN_RATE_LIMIT_TPM=${ADMIN_RATE_LIMIT_TPM:-1000000}
      - ADMIN_MAX_BUDGET=${ADMIN_MAX_BUDGET:-1000.0}
    volumes:
      - ./scripts/generate_keys.py:/app/scripts/generate_keys.py
    depends_on:
      postgres:
        condition: service_healthy
      litellm:
        condition: service_healthy
      backend:
        condition: service_started
    networks:
      - llm-network
    restart: "no"

  # ============================================================================
  #                           FRONTEND SERVICE
  # ============================================================================
  frontend:
    build:
      context: ./submodules/frontend
      dockerfile: Dockerfile
    container_name: llm-frontend
    ports:
      - "${BIND_ADDRESS:-127.0.0.1}:${FRONTEND_PORT:-3000}:80"
    deploy:
      resources:
        limits:
          cpus: "${FRONTEND_CPU_LIMIT:-1}"
          memory: ${FRONTEND_MEMORY_LIMIT:-512m}
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - llm-network

# ==============================================================================
#                               VOLUMES
# ==============================================================================
volumes:
  vllm-cache:
    name: llm-vllm-cache
  postgres-data:
    name: llm-postgres-data

# ==============================================================================
#                               NETWORKS
# ==============================================================================
networks:
  llm-network:
    name: llm-network
    driver: bridge
